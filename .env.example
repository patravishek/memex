# ── Provider selection ────────────────────────────────────────────────────────
# Which provider to use: "anthropic" | "openai" | "litellm"
# If not set, auto-detected from whichever key is present.
AI_PROVIDER=anthropic

# ── Anthropic (direct) ────────────────────────────────────────────────────────
ANTHROPIC_API_KEY=your_anthropic_api_key_here
ANTHROPIC_MODEL=claude-3-haiku-20240307

# ── OpenAI (direct) ───────────────────────────────────────────────────────────
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini

# ── LiteLLM (enterprise proxy) ────────────────────────────────────────────────
# Set AI_PROVIDER=litellm and fill these in.
# LITELLM_BASE_URL is your internal proxy, e.g. https://litellm.company.com
# LITELLM_MODEL must match a model configured in your LiteLLM deployment.
# LITELLM_TEAM_ID is optional — used if your proxy enforces team-based routing.
LITELLM_API_KEY=your_litellm_key_here
LITELLM_BASE_URL=https://litellm.your-company.com
LITELLM_MODEL=claude-3-haiku
LITELLM_TEAM_ID=
